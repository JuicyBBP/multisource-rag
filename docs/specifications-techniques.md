# SpÃ©cifications Techniques - MultiSource RAG System

## ğŸ“‹ Vue d'ensemble du projet

### Objectif
DÃ©velopper un systÃ¨me RAG (Retrieval-Augmented Generation) avancÃ© capable d'interroger intelligemment plusieurs sources de donnÃ©es (documents PDF, pages web, bases de donnÃ©es) pour fournir des rÃ©ponses contextuelles prÃ©cises avec citations.

### Proposition de valeur
- **Pour les recruteurs** : DÃ©montre la maÃ®trise des LLMs, architectures modernes, et capacitÃ© Ã  crÃ©er des systÃ¨mes production-ready
- **DiffÃ©renciation** : Multi-sources, interface Ã©lÃ©gante, dÃ©ploiement complet avec monitoring
- **ComplexitÃ© technique** : Architecture modulaire, optimisations GPU, gestion de contexte intelligente

---

## ğŸ¯ FonctionnalitÃ©s principales

### MVP (Version 1.0)
1. **Ingestion multi-sources**
   - Upload et parsing de documents PDF
   - Scraping de pages web via URL
   - Support de fichiers texte (.txt, .md)

2. **SystÃ¨me RAG de base**
   - Chunking intelligent des documents
   - Embeddings avec modÃ¨les optimisÃ©s
   - Recherche vectorielle avec ChromaDB
   - GÃ©nÃ©ration de rÃ©ponses avec citations

3. **Interface utilisateur**
   - Chat interface avec Streamlit
   - Historique de conversation
   - Affichage des sources utilisÃ©es
   - Upload drag & drop

4. **Backend API**
   - FastAPI avec endpoints REST
   - WebSocket pour streaming des rÃ©ponses
   - Gestion de sessions utilisateur

### Features avancÃ©es (Version 2.0)
- Multi-query generation pour amÃ©liorer le recall
- Reranking des rÃ©sultats avec Cross-Encoder
- Connexion Ã  bases de donnÃ©es (PostgreSQL)
- Recherche hybride (vectorielle + BM25)
- Cache intelligent des requÃªtes frÃ©quentes
- Export de conversations en PDF
- Comparaison de sources contradictoires

---

## ğŸ—ï¸ Architecture technique

### Stack technologique

#### Backend
- **Framework** : FastAPI 0.109+
- **LLM Provider** : OpenAI API / Anthropic Claude / Mistral API (configurable)
- **Vector Database** : ChromaDB 0.4.22+
- **Embeddings** :
  - Primaire : `sentence-transformers/all-MiniLM-L6-v2` (lÃ©ger, 384 dim)
  - Alternative : `BAAI/bge-small-en-v1.5` (meilleure qualitÃ©)
- **Document Processing** :
  - PyPDF2 / pdfplumber pour PDFs
  - BeautifulSoup4 + Playwright pour web scraping
  - python-docx pour Word
- **Cache** : Redis (optionnel pour production)

#### Frontend
- **Framework** : Streamlit 1.31+
- **Styling** : Custom CSS pour look professionnel
- **Charts** : Plotly pour visualisations

#### MLOps
- **Containerization** : Docker + Docker Compose
- **Monitoring** :
  - Logging structurÃ© avec Loguru
  - MÃ©triques custom (latence, coÃ»t tokens, satisfaction)
  - Prometheus + Grafana (optionnel)
- **Testing** : pytest + pytest-cov
- **CI/CD** : GitHub Actions

---

## ğŸ“Š Architecture des donnÃ©es

### Pipeline d'ingestion

```
Document Source â†’ Loader â†’ Text Splitter â†’ Embeddings â†’ Vector Store
                                  â†“
                            Metadata Extraction
                         (source, page, timestamp)
```

#### StratÃ©gie de chunking
- **MÃ©thode** : RecursiveCharacterTextSplitter
- **Chunk size** : 1000 caractÃ¨res
- **Overlap** : 200 caractÃ¨res (20%)
- **SÃ©parateurs** : `["\n\n", "\n", ". ", " ", ""]`

#### MÃ©tadonnÃ©es stockÃ©es
```json
{
  "source": "nom_fichier.pdf",
  "source_type": "pdf|web|txt|db",
  "page": 5,
  "chunk_id": "uuid",
  "timestamp": "2025-12-09T10:30:00Z",
  "char_count": 987,
  "url": "https://..." // si web
}
```

### Structure de la base vectorielle

**Collection ChromaDB** : `documents_collection`
- **Vecteurs** : Embeddings 384-dim (MiniLM) ou 768-dim (BGE)
- **Distance metric** : Cosine similarity
- **Index** : HNSW pour recherche rapide

---

## ğŸ”„ Flow de requÃªte RAG

### Ã‰tapes du pipeline

1. **RÃ©ception de la question utilisateur**
   - Validation et sanitization
   - DÃ©tection de la langue (pour support multilingue futur)

2. **Query Enhancement** (optionnel v2.0)
   - Reformulation de la question
   - GÃ©nÃ©ration de queries multiples
   - Expansion avec synonymes

3. **Retrieval**
   ```python
   # ParamÃ¨tres de recherche
   top_k = 5  # Nombre de chunks rÃ©cupÃ©rÃ©s
   similarity_threshold = 0.7  # Seuil de pertinence
   ```
   - Embedding de la question
   - Recherche vectorielle dans ChromaDB
   - Filtrage par score de similaritÃ©

4. **Reranking** (optionnel v2.0)
   - Cross-encoder pour reordonner les rÃ©sultats
   - ModÃ¨le : `cross-encoder/ms-marco-MiniLM-L-6-v2`

5. **Context Building**
   - AgrÃ©gation des chunks pertinents
   - DÃ©duplication des sources
   - Formatage avec mÃ©tadonnÃ©es

6. **Generation**
   - Construction du prompt avec contexte
   - Appel API LLM (streaming mode)
   - Post-processing de la rÃ©ponse

7. **Citation & Source Tracking**
   - Extraction des sources utilisÃ©es
   - Mapping vers documents originaux
   - Affichage des extraits pertinents

---

## ğŸ¨ Design du Prompt

### Template de base

```python
SYSTEM_PROMPT = """Tu es un assistant expert qui rÃ©pond aux questions en te basant
UNIQUEMENT sur les documents fournis.

RÃ¨gles importantes :
1. Cite toujours tes sources en utilisant [Source: nom_fichier, page X]
2. Si l'information n'est pas dans les documents, dis "Je ne trouve pas cette
   information dans les documents fournis"
3. Sois prÃ©cis et concis
4. Si plusieurs sources se contredisent, mentionne-le explicitement
"""

USER_PROMPT = """
Contexte fourni :
{context}

Question : {question}

RÃ©ponds en franÃ§ais de maniÃ¨re claire et structurÃ©e.
"""
```

### Gestion du contexte
- **Max tokens pour contexte** : 4000 tokens (~3000 mots)
- **StratÃ©gie si dÃ©passement** :
  - Prioriser les chunks avec meilleur score
  - Truncation intelligente sur phrases complÃ¨tes

---

## ğŸ”§ Configuration systÃ¨me

### Requirements GPU
- **Embeddings** : ~500MB VRAM (all-MiniLM-L6-v2 en local)
- **Alternative** : Utiliser API d'embeddings (OpenAI, Cohere) pour libÃ©rer GPU
- **Recommandation** : RTX 3070 8GB largement suffisante

### Requirements compute
```yaml
CPU: 4+ cores
RAM: 8GB minimum, 16GB recommandÃ©
Storage: 5GB pour environnement + documents
GPU: Optionnel (embeddings possibles sur CPU)
```

### Variables d'environnement
```bash
# LLM API
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-...
MISTRAL_API_KEY=...

# LLM Config
LLM_PROVIDER=openai  # openai|anthropic|mistral
LLM_MODEL=gpt-4-turbo-preview
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=1500

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cuda  # cuda|cpu

# ChromaDB
CHROMA_PERSIST_DIRECTORY=./data/chroma_db
CHROMA_COLLECTION_NAME=documents_collection

# App Config
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
RETRIEVAL_TOP_K=5
SIMILARITY_THRESHOLD=0.7

# Redis (optionnel)
REDIS_HOST=localhost
REDIS_PORT=6379
```

---

## ğŸ“ Structure du projet

```
MultiSource-RAG-System/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ specifications-techniques.md     # Ce fichier
â”‚   â”œâ”€â”€ planification-taches.md
â”‚   â””â”€â”€ architecture-diagram.png         # Ã€ crÃ©er
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                          # Point d'entrÃ©e FastAPI
â”‚   â”œâ”€â”€ config.py                        # Configuration & env vars
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schemas.py                   # Pydantic models
â”‚   â”‚   â””â”€â”€ database.py                  # ChromaDB setup
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_loader.py           # Ingestion multi-sources
â”‚   â”‚   â”œâ”€â”€ embeddings.py                # Embeddings service
â”‚   â”‚   â”œâ”€â”€ vector_store.py              # ChromaDB operations
â”‚   â”‚   â”œâ”€â”€ llm_service.py               # LLM API wrapper
â”‚   â”‚   â””â”€â”€ rag_pipeline.py              # Pipeline complet
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py                    # Endpoints REST
â”‚   â”‚   â””â”€â”€ websocket.py                 # Streaming responses
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ text_processing.py           # Chunking, cleaning
â”‚       â”œâ”€â”€ logger.py                    # Logging setup
â”‚       â””â”€â”€ metrics.py                   # Tracking mÃ©triques
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                           # Streamlit app
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ chat.py                      # Interface chat
â”‚   â”‚   â”œâ”€â”€ upload.py                    # Upload de documents
â”‚   â”‚   â””â”€â”€ sources.py                   # Affichage sources
â”‚   â””â”€â”€ styles/
â”‚       â””â”€â”€ custom.css                   # Styling
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_document_loader.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â”œâ”€â”€ test_rag_pipeline.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ chroma_db/                       # Vector DB persistence
â”‚   â”œâ”€â”€ uploaded_docs/                   # Documents uploadÃ©s
â”‚   â””â”€â”€ cache/                           # Query cache
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration_embeddings.ipynb
â”‚   â”œâ”€â”€ 02_chunking_strategies.ipynb
â”‚   â””â”€â”€ 03_evaluation_rag.ipynb
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.frontend
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ tests.yml
â”‚       â””â”€â”€ deploy.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ setup.py
â””â”€â”€ pytest.ini
```

---

## ğŸ§ª StratÃ©gie de test

### Tests unitaires
- Coverage minimum : 80%
- Tests pour chaque service indÃ©pendamment
- Mock des appels API externes

### Tests d'intÃ©gration
- Pipeline RAG end-to-end
- Endpoints API avec TestClient
- Simulation de documents de test

### Tests de performance
- Latence de requÃªte < 3 secondes
- Throughput : 10+ requÃªtes/seconde
- Memory footprint < 4GB

### MÃ©triques de qualitÃ© RAG
- **Faithfulness** : La rÃ©ponse est-elle fidÃ¨le au contexte ?
- **Answer Relevancy** : La rÃ©ponse rÃ©pond-elle Ã  la question ?
- **Context Precision** : Les chunks rÃ©cupÃ©rÃ©s sont-ils pertinents ?
- **Context Recall** : Tous les chunks nÃ©cessaires sont-ils rÃ©cupÃ©rÃ©s ?

---

## ğŸš€ Plan de dÃ©ploiement

### Environnements

1. **Development**
   - Local avec hot reload
   - Base ChromaDB en mÃ©moire
   - Logs verbose

2. **Staging** (optionnel)
   - Docker Compose
   - ChromaDB persistente
   - Tests d'intÃ©gration automatiques

3. **Production**
   - **Option 1** : Docker Compose sur VPS
   - **Option 2** : Kubernetes (overkill pour portfolio)
   - **Option 3** : Railway / Render / Fly.io
   - HTTPS avec Let's Encrypt
   - Monitoring activÃ©

### DÃ©ploiement du frontend
- Streamlit Community Cloud (gratuit)
- Ou avec le backend sur mÃªme serveur

---

## ğŸ“ˆ Monitoring & Observability

### Logs structurÃ©s
```python
{
  "timestamp": "2025-12-09T10:30:00Z",
  "level": "INFO",
  "service": "rag_pipeline",
  "event": "query_processed",
  "user_id": "abc123",
  "question": "...",
  "retrieved_docs": 5,
  "generation_time": 2.3,
  "total_tokens": 1200,
  "cost_usd": 0.0024
}
```

### MÃ©triques Ã  tracker
- Nombre de requÃªtes / jour
- Latence moyenne par composant
- CoÃ»t API par requÃªte
- Taux d'erreur
- Distribution des sources utilisÃ©es
- Satisfaction utilisateur (feedback thumbs up/down)

### Dashboard Grafana (optionnel)
- Graphiques de latence
- CoÃ»t cumulÃ©
- Volume de documents
- Taux de cache hit

---

## ğŸ”’ SÃ©curitÃ©

### ConsidÃ©rations
1. **API Keys** : Jamais en dur, toujours via .env
2. **Rate limiting** : 100 requÃªtes/heure par IP
3. **Input validation** : Sanitization des uploads
4. **File size limits** : Max 10MB par document
5. **Content moderation** : Filtrage des prompts malveillants
6. **CORS** : Configuration restrictive en production

### Secrets management
- `.env` pour local
- Secrets manager en production (AWS Secrets, Doppler)

---

## ğŸ’° Estimation des coÃ»ts

### CoÃ»ts d'API (par 1000 requÃªtes)

**Embeddings** (si API externe)
- OpenAI `text-embedding-3-small` : ~$0.02/1M tokens â†’ nÃ©gligeable
- En local (gratuit) : All-MiniLM-L6-v2

**LLM Generation**
- GPT-4 Turbo : $0.01/1K input tokens, $0.03/1K output â†’ ~$0.05/requÃªte
- Claude Sonnet 3.5 : $0.003/1K input, $0.015/1K output â†’ ~$0.025/requÃªte
- Mistral Small : ~$0.002/1K tokens â†’ ~$0.01/requÃªte

**Estimation rÃ©aliste pour dÃ©mo portfolio** : $5-10/mois avec usage modÃ©rÃ©

---

## ğŸ“š Documentation pour le portfolio

### README.md Ã  inclure
- Badge CI/CD status
- DÃ©mo GIF/vidÃ©o
- Architecture diagram
- Instructions d'installation en 3 commandes
- Exemples de questions/rÃ©ponses
- MÃ©triques de performance
- Technologies utilisÃ©es avec badges

### Ã‰lÃ©ments impressionnants Ã  montrer
1. **Architecture claire** : Diagram avec draw.io
2. **MÃ©triques concrÃ¨tes** : "RÃ©pond en moyenne en 2.3s avec prÃ©cision de 87%"
3. **DÃ©mo live** : URL fonctionnelle Ã  tester
4. **Code quality** : Tests, linting, type hints
5. **Production-ready** : Docker, monitoring, CI/CD

---

## ğŸ“ CompÃ©tences dÃ©montrÃ©es

âœ… **LLMs & RAG** : Architecture moderne, prompt engineering
âœ… **Vector Databases** : ChromaDB, embeddings, similarity search
âœ… **APIs** : FastAPI, WebSockets, REST design
âœ… **Frontend** : Streamlit, UX design
âœ… **MLOps** : Docker, CI/CD, monitoring
âœ… **Software Engineering** : Architecture modulaire, tests, documentation
âœ… **Optimisation** : Gestion GPU, caching, performance

---

## ğŸ“‹ Checklist finale avant prÃ©sentation

- [ ] Code propre avec type hints et docstrings
- [ ] Tests avec coverage > 80%
- [ ] Documentation README complÃ¨te avec exemples
- [ ] DÃ©mo dÃ©ployÃ©e et accessible en ligne
- [ ] Architecture diagram professionnel
- [ ] MÃ©triques de performance mesurÃ©es
- [ ] Code sur GitHub avec historique de commits propre
- [ ] License open-source (MIT)
- [ ] CHANGELOG.md avec versions
- [ ] VidÃ©o dÃ©mo de 2-3 minutes (optionnel mais fort impact)

---

**Version** : 1.0
**DerniÃ¨re mise Ã  jour** : 2025-12-09
**Auteur** : [Votre nom]
**ComplexitÃ© estimÃ©e** : IntermÃ©diaire Ã  AvancÃ©
**Temps de dÃ©veloppement estimÃ©** : 2-3 semaines Ã  temps plein
